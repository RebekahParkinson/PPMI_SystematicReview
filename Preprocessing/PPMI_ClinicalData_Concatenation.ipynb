{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI CLINICAL DATA CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix, PatsyError\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Transposing Blood Chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloodChem_InitialRows:  217477 BloodChem_InitialColumns:  23\n",
      "BloodChem_AfterDroppedRows:  217477 BloodChem_AfterDroppedColumns:  4\n",
      "BloodChem_AfterTranspose_Rows:  217477 BloodChem_AfterTranspose_Columns:  45\n"
     ]
    }
   ],
   "source": [
    "BloodChem = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/IDA_PPMI_database_12Nov2023/Blood/Blood_Chemistry___Hematology-Archived_12Nov2023.csv\"  # File relative path\n",
    "df_BloodChem = pd.read_csv(BloodChem)\n",
    "print(\"BloodChem_InitialRows: \",len(df_BloodChem),\"BloodChem_InitialColumns: \",len(df_BloodChem.columns))\n",
    "columns_to_drop = [\"PAG_NAME\", \"LCOLLDT\",\"COLLTM\",\"LRECDT\",\"RECTM\",\"LRPTDT\",\"RPTTM\",\"LABCODE\",\"LGROUP\",\"LTSTCODE\",\"LVISTYPE\",\"LSIUNIT\",\"LSILORNG\",\"LSIHIRNG\",\"LUSRES\",\"LUSUNIT\",\"LUSLORNG\",\"LUSHIRNG\",\"LRESFLG\"]\n",
    "df_BloodChem = df_BloodChem.drop(columns=columns_to_drop)\n",
    "# df_BloodChem = df_BloodChem.drop([\"PAG_NAME\", \"LCOLLDT\",\"COLLTM\",\"LRECDT\",\"RECTM\",\"LRPTDT\",\"RPTTM\",\"LABCODE\",\"LGROUP\",\"LTSTCODE\",\"LVISTYPE\",\"LSIUNIT\",\"LSILORNG\",\"LSIHIRNG\",\"LUSRES\",\"LUSUNIT\",\"LUSLORNG\",\"LUSHIRNG\",\"LRESFLG\", axis=\"columns\") #Alternative method\n",
    "print(\"BloodChem_AfterDroppedRows: \",len(df_BloodChem),\"BloodChem_AfterDroppedColumns: \",len(df_BloodChem.columns))\n",
    "\n",
    "## Tranpspose the 'LTSTNAME' and 'LSIRES' columns and compress to fit within the same row as defined by corresponding 'PATNO' AND 'EVENT_ID'\n",
    "BloodChem_Transposed = df_BloodChem.pivot_table(index=['PATNO', 'EVENT_ID'], columns='LTSTNAME', values='LSIRES', aggfunc='first')\n",
    "BloodChem_Transposed.reset_index(inplace=True)\n",
    "# pivoted_df_BloodChem.fillna(0, inplace=True) # Fill missing values with 0 (or any other appropriate value)\n",
    "# pivoted_df = pivoted_df_BloodChem.loc[:, ~pivoted_df_BloodChem.columns.duplicated()] # Remove the duplicate 'PATNO' and 'EVENT_ID' columns\n",
    "BloodChem_Transposed = pd.concat([df_BloodChem, BloodChem_Transposed], axis=1) # Merge the pivoted DataFrame with the original DataFrame based on the index\n",
    "BloodChem_Transposed= BloodChem_Transposed.drop([\"LTSTNAME\", \"LSIRES\"], axis=\"columns\") #Drop the unneccessary cols\n",
    "\n",
    "#Drop the PATNO and EVENT_ID cols that were duplicated in the merge step\n",
    "BloodChem_Transposed.columns.values[0] = \"DROP_THIS\"\n",
    "BloodChem_Transposed.columns.values[1] = \"DROP_THIS\"\n",
    "BloodChem_Transposed = BloodChem_Transposed.drop(\"DROP_THIS\", axis=1)\n",
    "\n",
    "# Replace 'SC' with 'BL' to pool for consistency\n",
    "BloodChem_Transposed['EVENT_ID'] = BloodChem_Transposed['EVENT_ID'].replace('SC', 'BL') \n",
    "BloodChem_Transposed.to_csv(\"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Code_Output/BloodChem_Transposed.csv\")\n",
    "print(\"BloodChem_AfterTranspose_Rows: \",len(BloodChem_Transposed),\"BloodChem_AfterTranspose_Columns: \",len(BloodChem_Transposed.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Merge all datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAA_Rows:  1362 SAA_Columns:  47\n",
      "SAA_prodromal_Rows:  2823 SAA_prodromal_Columns:  47\n",
      "SAA_final_Rows:  2819 SAA_final_Columns:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/gx6gh43j57b1n5mbc197djbh0000gn/T/ipykernel_11895/661793076.py:25: DtypeWarning: Columns (10,11,20,21,23,27,28,29,117,122) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_Curated = pd.read_csv(Curated_Data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuratedData_Rows:  10152 CuratedData_Columns:  155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/gx6gh43j57b1n5mbc197djbh0000gn/T/ipykernel_11895/661793076.py:27: DtypeWarning: Columns (2,3,5,6,9,10,15,33,34,36,40,41,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  BloodChem_Transposed = pd.read_csv(BloodChem_Transposed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinalConcat_Rows:  222285 FinalConcat_Columns:  244\n",
      "FinalConcat_Rows:  222285 FinalConcat_Columns:  244\n"
     ]
    }
   ],
   "source": [
    "SAA = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/IDA_PPMI_database_12Nov2023/Biospecimen/Biospecimen_Analysis/SAA_Biospecimen_Analysis_Results_12Nov2023.csv\"\n",
    "SAA_prodromal = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Tiers3 Data/SAA_Internal_20231220.csv\"\n",
    "Curated_Data = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/IDA_PPMI_database_12Nov2023/Curated_Data_Cuts/PPMI_Curated_Data_Cut_Public_20230612_rev.csv\"\n",
    "BloodChem_Transposed = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Code_Output/BloodChem_Transposed.csv\"\n",
    "DATSCAN_prodromal = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Tiers3 Data/PPMI_SEQUESTERED_SPECT&PET_CSVs_20231121/inv_spect_sbr_sequestered_merge_20231121.csv\"\n",
    "#DATSCAN_CAUDATE_R, DATSCAN_CAUDATE_L, DATSCAN_PUTAMEN_R, DATSCAN_PUTAMEN_L\n",
    "\n",
    "# inv_spect_vi_sequestered_merge_20231121\n",
    "# DATSCAN_VISINTRP = positive or negative\n",
    "\n",
    "#Merge prodromal SAA with other SAA\n",
    "df_SAA = pd.read_csv(SAA)\n",
    "print(\"SAA_Rows: \",len(df_SAA),\"SAA_Columns: \",len(df_SAA.columns))\n",
    "df_SAA_prodromal = pd.read_csv(SAA_prodromal)\n",
    "print(\"SAA_prodromal_Rows: \",len(df_SAA_prodromal),\"SAA_prodromal_Columns: \",len(df_SAA_prodromal.columns))\n",
    "if list(df_SAA.columns) == list(df_SAA_prodromal.columns):\n",
    "    unique_rows = df_SAA_prodromal[~df_SAA_prodromal.set_index(['PATNO', 'CLINICAL_EVENT']).index.isin(df_SAA.set_index(['PATNO', 'CLINICAL_EVENT']).index)]\n",
    "    df_SAA_final = pd.concat([df_SAA, unique_rows]) #TODO: n.b. different functions... pd.merge instead \n",
    "    df_SAA_final.to_csv(\"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Code_Output/Final_SAA.csv\")\n",
    "else:\n",
    "    print(\"Error: The columns of the original and new dataframes do not match.\")\n",
    "print(\"SAA_final_Rows: \",len(df_SAA_final),\"SAA_final_Columns: \",len(df_SAA_final.columns))\n",
    "\n",
    "# Merge BloodChem with Curated Dataframe,also based on duplicated values in \"PATNO\" and \"EVENT_ID\"\n",
    "df_Curated = pd.read_csv(Curated_Data)\n",
    "print(\"CuratedData_Rows: \",len(df_Curated),\"CuratedData_Columns: \",len(df_Curated.columns))\n",
    "BloodChem_Transposed = pd.read_csv(BloodChem_Transposed)\n",
    "df_FinalConcat = pd.merge(df_Curated, BloodChem_Transposed, on=[\"PATNO\",\"EVENT_ID\"], how='outer')\n",
    "\n",
    "# Merge the asyn CSF data with Curated Daframe,also based on duplicated values in \"PATNO\" and \"EVENT_ID\"\n",
    "df_SAA_final.columns = ['EVENT_ID' if col == 'CLINICAL_EVENT' else col for col in df_SAA.columns] \n",
    "df_FinalConcat = pd.merge(df_FinalConcat, df_SAA_final, on=[\"PATNO\",\"EVENT_ID\"], how='outer')\n",
    "print(\"FinalConcat_Rows: \",len(df_FinalConcat),\"FinalConcat_Columns: \",len(df_FinalConcat.columns))\n",
    "\n",
    "# Merge in (existing) neuro data with prodromal patient data\n",
    "df_DATSCAN_prodromal = pd.read_csv(DATSCAN_prodromal)\n",
    "df_DATSCAN_prodromal['DATSCAN_PUTAMEN_R'] = pd.to_numeric(df_DATSCAN_prodromal['DATSCAN_PUTAMEN_R'], errors='coerce').astype(float)\n",
    "df_DATSCAN_prodromal['DATSCAN_PUTAMEN_L'] = pd.to_numeric(df_DATSCAN_prodromal['DATSCAN_PUTAMEN_L'], errors='coerce').astype(float)\n",
    "df_DATSCAN_prodromal['mean_putamen'] = df_DATSCAN_prodromal[['DATSCAN_PUTAMEN_R', 'DATSCAN_PUTAMEN_L']].mean(axis=1)\n",
    "df_DATSCAN_selected = df_DATSCAN_prodromal[['PATNO', 'EVENT_ID', 'mean_putamen','DATSCAN_CAUDATE_R', 'DATSCAN_CAUDATE_L', 'DATSCAN_PUTAMEN_R', 'DATSCAN_PUTAMEN_L']]\n",
    "df_FinalConcat = pd.merge(df_FinalConcat, df_DATSCAN_selected, on=['PATNO', 'EVENT_ID'], how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Replacing existing columns with the new ones where values are not NaN\n",
    "for column in ['mean_putamen','DATSCAN_CAUDATE_R', 'DATSCAN_CAUDATE_L','DATSCAN_PUTAMEN_R', 'DATSCAN_PUTAMEN_L']:\n",
    "    df_FinalConcat[column] = df_FinalConcat[column].combine_first(df_FinalConcat[column + '_new'])\n",
    "    df_FinalConcat.drop(columns=[column + '_new'], inplace=True)\n",
    "print(\"FinalConcat_Rows: \",len(df_FinalConcat),\"FinalConcat_Columns: \",len(df_FinalConcat.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC step: checking primdiag is consistent across patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comparisons: 1335\n",
      "Number of discrepancies: 0\n",
      "All PRIMDIAG values match between the dataframes.\n"
     ]
    }
   ],
   "source": [
    "# Merge the dataframes on 'PATNO' and 'EVENT_ID', and keep the 'PRIMDIAG' columns\n",
    "Prodromal_Elligibility = \"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/IDA_PPMI_database_12Nov2023/Prodromal/Prodromal_Diagnostic_Questionnaire-Archived_12Nov2023.csv\"\n",
    "df_Prodromal_Elligibility = pd.read_csv(Prodromal_Elligibility)\n",
    "merged_df = pd.merge(\n",
    "    df_Prodromal_Elligibility[['PATNO', 'EVENT_ID', 'PRIMDIAG']], \n",
    "    df_FinalConcat[['PATNO', 'EVENT_ID', 'PRIMDIAG']], \n",
    "    on=['PATNO', 'EVENT_ID'], \n",
    "    suffixes=('_eligibility', '_final')\n",
    ")\n",
    "# Exclude rows with NaN values in either PRIMDIAG column\n",
    "merged_df = merged_df.dropna(subset=['PRIMDIAG_eligibility', 'PRIMDIAG_final'])\n",
    "# Identify rows where the PRIMDIAG values differ\n",
    "discrepancies = merged_df[merged_df['PRIMDIAG_eligibility'] != merged_df['PRIMDIAG_final']]\n",
    "# Determine the amount of discrepancies out of the total comparisons\n",
    "total_comparisons = merged_df.shape[0]\n",
    "discrepancy_count = discrepancies.shape[0]\n",
    "\n",
    "print(f\"Total comparisons: {total_comparisons}\")\n",
    "print(f\"Number of discrepancies: {discrepancy_count}\")\n",
    "\n",
    "if not discrepancies.empty:\n",
    "    print(\"There are discrepancies in PRIMDIAG values between the dataframes:\")\n",
    "    print(discrepancies.to_string())  # Ensures all rows are printed\n",
    "else:\n",
    "    print(\"All PRIMDIAG values match between the dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cohort categorisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sporadic' 'Hyposmia' 'Healthy Control' 'PRKN' 'LRRK2' 'GBA'\n",
      " 'RBD + Hyposmia' 'RBD' 'LRRK2 + Hyposmia' 'PINK1' 'SNCA + Hyposmia'\n",
      " 'LRRK2 + GBA' 'SNCA' 'GBA + Hyposmia' 'LRRK2 + GBA + Hyposmia'\n",
      " 'GBA + RBD + Hyposmia' nan]\n",
      "['Sporadic' 'Hyposmia' 'Healthy Control' 'Genetic' 'RBD +/- Hyposomia' nan]\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping for subgroups to broader categories\n",
    "df_FinalConcat['subgroup_simplified'] = df_FinalConcat['subgroup']\n",
    "subgroup_mapping = {\n",
    "    #Genetic\n",
    "    'PRKN': 'Genetic', \n",
    "    'LRRK2': 'Genetic',\n",
    "    'LRRK2 + GBA': 'Genetic',  \n",
    "    'GBA': 'Genetic',\n",
    "    'SNCA': 'Genetic', \n",
    "    'PINK1': 'Genetic', \n",
    "    'LRRK2 + Hyposmia': 'Genetic',\n",
    "    'SNCA + Hyposmia': 'Genetic', \n",
    "    'GBA + Hyposmia': 'Genetic',\n",
    "    'LRRK2 + GBA + Hyposmia': 'Genetic', \n",
    "    'GBA + RBD + Hyposmia': 'Genetic',\n",
    "    #Prodromal\n",
    "    'RBD': 'RBD +/- Hyposomia',\n",
    "    'RBD + Hyposmia': 'RBD +/- Hyposomia'\n",
    "    # 'Hyposmia': 'Hyposomia +/- RBD'\n",
    "\n",
    "}\n",
    "color_dict = {\"Genetic\":\"green\",\n",
    "              \"Hyposomia\":\"purple\",\n",
    "              \"RBD +/- Hyposomia\": \"pink\",\n",
    "              \"Sporadic\":\"blue\",\n",
    "              \"Healthy Control\":\"black\"\n",
    "              }\n",
    "# Apply mapping to the 'subgroup' column\n",
    "df_FinalConcat['subgroup_simplified'] = df_FinalConcat['subgroup_simplified'].replace(subgroup_mapping)\n",
    "print(df_FinalConcat['subgroup'].unique())\n",
    "print(df_FinalConcat['subgroup_simplified'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Specific wrangling and creation of additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat_Rows:  222285 Concat_Columns:  248\n",
      "Concat_Rows:  222230 Concat_Columns:  250\n",
      "      PATNO mean_putamen\n",
      "0  100001.0        1.080\n",
      "1  100001.0        1.040\n",
      "2  100001.0        0.825\n",
      "3  100002.0        0.530\n",
      "4  100002.0        0.590\n"
     ]
    }
   ],
   "source": [
    "#Dealing with the updrs behavioural data\n",
    "df = df_FinalConcat.dropna(subset=['updrs1_score']) # remove rows that contain a NaN in updrs1_score\n",
    "df_FinalConcat['updrs2_score'].fillna(0, inplace=True) # make NaNs in updrs2_score ==0\n",
    "df_FinalConcat['updrs4_score'].fillna(0, inplace=True) # make NaNs in updrs4_score ==0\n",
    "# df_FinalConcat['updrs3_score_on'].fillna(df_FinalConcat['updrs3_score'], inplace=True) # If updrs3_score_on contains a NaN, then merge the value in column updrs3_score from the same row\n",
    "# df_FinalConcat= df_FinalConcat.drop([\"updrs3_score\", \"updrs_totscore\", \"updrs_totscore_on\"], axis=\"columns\") # remove updrs3_score\n",
    "df_FinalConcat['updrs3_score_on'].fillna(0, inplace=True) # make NaNs in updrs3_score ==0\n",
    "df_FinalConcat['updrs1_score'] = pd.to_numeric(df_FinalConcat['updrs1_score'], errors='coerce') # 'coerce' converts non-numeric values to NaN\n",
    "df_FinalConcat['updrs2_score'] = pd.to_numeric(df_FinalConcat['updrs2_score'], errors='coerce')  \n",
    "df_FinalConcat['updrs3_score_on'] = pd.to_numeric(df_FinalConcat['updrs3_score_on'], errors='coerce')\n",
    "df_FinalConcat['updrs4_score'] = pd.to_numeric(df_FinalConcat['updrs4_score'], errors='coerce')\n",
    "df_FinalConcat['updrs_totscore_I-III'] = df_FinalConcat[['updrs1_score', 'updrs2_score', 'updrs3_score_on']].sum(axis=1) #Sum the first 3 updrs score categories in new column\n",
    "df_FinalConcat['updrs_totscore_I-IV'] = df_FinalConcat[['updrs1_score', 'updrs2_score', 'updrs3_score_on','updrs4_score']].sum(axis=1) #Sum all the updrs score categories in new column\n",
    "\n",
    "#Dealing with the FmaxRep asynuclein assay data\n",
    "df_FinalConcat['FmaxRep_av'] = df_FinalConcat[['FmaxRep1', 'FmaxRep2', 'FmaxRep3']].mean(axis=1) \n",
    "print(\"Concat_Rows: \",len(df_FinalConcat),\"Concat_Columns: \",len(df_FinalConcat.columns))\n",
    "\n",
    "# Make NLR column\n",
    "df_FinalConcat['Neutrophils'] = pd.to_numeric(df_FinalConcat['Neutrophils'], errors='coerce')\n",
    "df_FinalConcat['Lymphocytes'] = pd.to_numeric(df_FinalConcat['Lymphocytes'], errors='coerce')\n",
    "df_FinalConcat['NLR'] = df_FinalConcat['Neutrophils'] / df_FinalConcat['Lymphocytes']\n",
    "# print(\"1FinalConcat_Rows: \",len(df_FinalConcat),\"1FinalConcat_Columns: \",len(df_FinalConcat.columns))\n",
    "# df_FinalConcat.to_csv(\"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Code_Output/Final_Merge.csv\")\n",
    "\n",
    "#quantify EVENTIDS in number of months\n",
    "unwanted_visits = ['ST','PW','RETEST','RS1','U01']\n",
    "df_FinalConcat = df_FinalConcat[~df_FinalConcat['EVENT_ID'].isin(unwanted_visits)]\n",
    "event_id_map = {\n",
    "    'BL': 0,\n",
    "    'V01': 3,\n",
    "    'V02': 6,\n",
    "    'V03': 9,\n",
    "    'V04': 12,\n",
    "    'V05': 15,\n",
    "    'V06': 18,\n",
    "    'V07': 21,\n",
    "    'V08': 24,\n",
    "    'V09': 27,\n",
    "    'V10': 30,\n",
    "    'V12': 36,\n",
    "    'V13': 39,\n",
    "    'V14': 42,\n",
    "    'V15': 45,\n",
    "    'V16': 48,\n",
    "    'V17': 51,\n",
    "    'V18': 54,\n",
    "    'V19': 57\n",
    "}\n",
    "\n",
    "df_FinalConcat['EVENT_NUM'] = df_FinalConcat['EVENT_ID'].map(event_id_map)\n",
    "print(\"Concat_Rows: \",len(df_FinalConcat),\"Concat_Columns: \",len(df_FinalConcat.columns))\n",
    "\n",
    "df_FinalConcat.to_csv(\"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Code_Output/FinalConcat.csv\")\n",
    "\n",
    "####PHENOCONVERTED PX\n",
    "PRIMDIAG_1 = df_FinalConcat[df_FinalConcat['PRIMDIAG'] == 1.0]\n",
    "unique_patnos = PRIMDIAG_1['PATNO'].unique()\n",
    "Phenoconverted = df_FinalConcat[df_FinalConcat['PATNO'].isin(unique_patnos)]\n",
    "Phenoconverted.to_csv(\"/Volumes/PARK2023-Q5758/SAA Systematic Review (Human Data)/Code_Output/Phenoconverted.csv\")\n",
    "\n",
    "#######Quantify Time Since Diagnosis (v1)\n",
    "# df_FinalConcat['age_at_visit'] = pd.to_numeric(df_FinalConcat['age_at_visit'], errors='coerce').astype(float)\n",
    "# df_FinalConcat['agediag'] = pd.to_numeric(df_FinalConcat['agediag'], errors='coerce').astype(float)\n",
    "# df_FinalConcat['ageonset'] = pd.to_numeric(df_FinalConcat['ageonset'], errors='coerce').astype(float)\n",
    "\n",
    "# df_FinalConcat['years_since_diagnosis'] = df_FinalConcat['age_at_visit'] - df_FinalConcat['agediag']\n",
    "# df_FinalConcat['years_since_onset'] = df_FinalConcat['age_at_visit'] - df_FinalConcat['ageonset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      PATNO  mean_putamen\n",
      "0  100001.0         1.080\n",
      "1  100001.0         1.040\n",
      "2  100001.0         0.825\n",
      "3  100002.0         0.530\n",
      "4  100002.0         0.590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######Normalise the putamen values by age and sex\n",
    "#mean_putamen is getting affected\n",
    "#still not working...\n",
    "# Create a summary DataFrame with one row per patient\n",
    "# Convert mean_putamen to numeric, coerce errors to NaN\n",
    "# Convert mean_putamen to numeric, coerce errors to NaN\n",
    "df_FinalConcat['mean_putamen'] = pd.to_numeric(df_FinalConcat['mean_putamen'], errors='coerce')\n",
    "\n",
    "# Define age bins\n",
    "age_bins = list(range(0, 105, 5))  # 0-5, 5-10, ..., 100-105 (Define age ranges by 5 years)\n",
    "df_FinalConcat['age_group'] = pd.cut(df_FinalConcat['age_at_visit'], bins=age_bins, right=False)\n",
    "\n",
    "# Create a summary DataFrame with one row per patient\n",
    "df_summary = df_FinalConcat.sort_values(by=['PATNO', 'age_at_visit']).drop_duplicates(subset=['PATNO'], keep='last')\n",
    "\n",
    "# Filter Healthy Control subgroup\n",
    "healthy_controls = df_summary[df_summary['subgroup'] == 'Healthy Control']\n",
    "\n",
    "# Calculate mean and std for each age group and sex in Healthy Control\n",
    "age_sex_group_stats = healthy_controls.groupby(['age_group', 'SEX_x'])['mean_putamen'].agg(['mean', 'std']).reset_index()\n",
    "age_sex_group_stats.rename(columns={'mean': 'mean_putamen_healthy', 'std': 'std_putamen_healthy'}, inplace=True)\n",
    "\n",
    "# Merge statistics back into the summary DataFrame\n",
    "df_summary = df_summary.merge(age_sex_group_stats, on=['age_group', 'SEX_x'], how='left')\n",
    "\n",
    "\n",
    "# Function for normalizing mean_putamen\n",
    "def normalise_putamen(row):\n",
    "    if pd.isnull(row['mean_putamen']) or pd.isnull(row['mean_putamen_healthy']) or pd.isnull(row['std_putamen_healthy']):\n",
    "        return np.nan\n",
    "    return (row['mean_putamen'] - row['mean_putamen_healthy']) / row['std_putamen_healthy']\n",
    "\n",
    "# Apply normalization\n",
    "df_summary['normalised_mean_putamen'] = df_summary.apply(normalise_putamen, axis=1)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_summary.drop(columns=['age_group', 'mean_putamen_healthy', 'std_putamen_healthy'], inplace=True)\n",
    "\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(df_FinalConcat[['PATNO', 'mean_putamen']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'normalised_mean_putamen_x', 'percent_mean_putamen_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m df_summary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent_mean_putamen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df_summary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalised_mean_putamen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m min_normalised) \u001b[38;5;241m/\u001b[39m (max_normalised \u001b[38;5;241m-\u001b[39m min_normalised)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Merge normalized data back into the original DataFrame\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m df_FinalConcat \u001b[38;5;241m=\u001b[39m \u001b[43mdf_FinalConcat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_summary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPATNO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormalised_mean_putamen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpercent_mean_putamen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPATNO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Check the resulting DataFrame\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_summary[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPATNO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_putamen\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:9848\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9829\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9830\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   9831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9844\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   9845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9846\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m-> 9848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9849\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9857\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9858\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:158\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    144\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    145\u001b[0m         left,\n\u001b[1;32m    146\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    157\u001b[0m     )\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    805\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m--> 807\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:759\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    756\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[1;32m    757\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[0;32m--> 759\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    768\u001b[0m         join_index,\n\u001b[1;32m    769\u001b[0m         left_indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    775\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:2608\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[0;34m(left, right, suffixes)\u001b[0m\n\u001b[1;32m   2606\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[0;32m-> 2608\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[1;32m   2609\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2610\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2611\u001b[0m     )\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[0;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'normalised_mean_putamen_x', 'percent_mean_putamen_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "\n",
    "######Normalise the putamen values by age and sex\n",
    "#mean_putamen is getting affected\n",
    "#still not working...\n",
    "# Create a summary DataFrame with one row per patient\n",
    "# Convert mean_putamen to numeric, coerce errors to NaN\n",
    "df_FinalConcat['mean_putamen'] = pd.to_numeric(df_FinalConcat['mean_putamen'], errors='coerce')\n",
    "\n",
    "# Define age bins\n",
    "age_bins = list(range(20, 90, 10))  # 0-5, 5-10, ..., 100-105 (Define age ranges by 5 years)\n",
    "df_FinalConcat['age_group'] = pd.cut(df_FinalConcat['age_at_visit'], bins=age_bins, right=False)\n",
    "\n",
    "# Create a summary DataFrame with one row per patient\n",
    "df_summary = df_FinalConcat.sort_values(by=['PATNO', 'age_at_visit']).drop_duplicates(subset=['PATNO'], keep='last')\n",
    "\n",
    "# Filter Healthy Control subgroup\n",
    "healthy_controls = df_summary[df_summary['subgroup'] == 'Healthy Control']\n",
    "\n",
    "# Calculate mean and std for each age group and sex in Healthy Control\n",
    "age_sex_group_stats = healthy_controls.groupby(['age_group', 'SEX_x'])['mean_putamen'].agg(['mean', 'std']).reset_index()\n",
    "age_sex_group_stats.rename(columns={'mean': 'mean_putamen_healthy', 'std': 'std_putamen_healthy'}, inplace=True)\n",
    "\n",
    "# Merge statistics back into the summary DataFrame\n",
    "df_summary = df_summary.merge(age_sex_group_stats, on=['age_group', 'SEX_x'], how='left')\n",
    "\n",
    "# Function for normalizing mean_putamen\n",
    "def normalise_putamen(row):\n",
    "    if pd.isnull(row['mean_putamen']) or pd.isnull(row['mean_putamen_healthy']) or pd.isnull(row['std_putamen_healthy']):\n",
    "        return np.nan\n",
    "    return (row['mean_putamen'] - row['mean_putamen_healthy']) / row['std_putamen_healthy']\n",
    "\n",
    "# Apply normalization\n",
    "df_summary['normalised_mean_putamen'] = df_summary.apply(normalise_putamen, axis=1)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_summary.drop(columns=['age_group', 'mean_putamen_healthy', 'std_putamen_healthy'], inplace=True)\n",
    "\n",
    "# Normalize between 0-1\n",
    "min_normalised = df_summary['normalised_mean_putamen'].min()\n",
    "max_normalised = df_summary['normalised_mean_putamen'].max()\n",
    "df_summary['percent_mean_putamen'] = (df_summary['normalised_mean_putamen'] - min_normalised) / (max_normalised - min_normalised)\n",
    "\n",
    "\n",
    "# Merge normalized data back into the original DataFrame\n",
    "df_FinalConcat = df_FinalConcat.merge(df_summary[['PATNO', 'normalised_mean_putamen', 'percent_mean_putamen']], on='PATNO', how='left')\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(df_summary[['PATNO', 'mean_putamen']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########Quantify Time Since Diagnosis (v2 - not final yet)\n",
    "# df_FinalConcat['age_at_visit'] = pd.to_numeric(df_FinalConcat['age_at_visit'], errors='coerce').astype(float)\n",
    "# df_FinalConcat['agediag'] = pd.to_numeric(df_FinalConcat['agediag'], errors='coerce').astype(float)\n",
    "# df_FinalConcat['ageonset'] = pd.to_numeric(df_FinalConcat['ageonset'], errors='coerce').astype(float)\n",
    "\n",
    "# df_FinalConcat['years_since_diagnosis'] = df_FinalConcat['age_at_visit'] - df_FinalConcat['agediag']\n",
    "# df_FinalConcat['years_since_onset'] = df_FinalConcat['age_at_visit'] - df_FinalConcat['ageonset']\n",
    "\n",
    "\n",
    "# #For sporadic patients\n",
    "# df_FinalConcat['years_since_diagnosis'] = df_FinalConcat.apply(\n",
    "#     lambda row: row['age_at_visit'] - row['agediag'] if row['subgroup'] == 'Sporadic' or 'Genetic' else None,\n",
    "#     axis=1\n",
    "# )\n",
    "# # For prodromal patients, calculate the diagnosis point based on first 'PRIMDIAG' of 01\n",
    "# def calculate_years_since_diagnosis(row, primdiag_dates):\n",
    "#     if row['subgroup'] in ['Hyposomia', 'RBD +/- Hyposomia']:\n",
    "#         diagnosis_age = primdiag_dates.get(row['PATNO'], None)\n",
    "#         if diagnosis_age:\n",
    "#             return row['age_at_visit'] - diagnosis_age\n",
    "#     return None\n",
    "# # Find the first occurrence of 'PRIMDIAG' == 01 for each patient\n",
    "# prodromal_patients = df_FinalConcat[df_FinalConcat['PRIMDIAG'] == 1]\n",
    "# primdiag_dates = prodromal_patients.groupby('PATNO')['age_at_visit'].min().to_dict()\n",
    "# # Apply the function to calculate years since diagnosis for prodromal patients\n",
    "# df_FinalConcat['years_since_diagnosis'] = df_FinalConcat.apply(\n",
    "#     lambda row: calculate_years_since_diagnosis(row, primdiag_dates) if pd.isna(row['years_since_diagnosis']) else row['years_since_diagnosis'],\n",
    "#     axis=1\n",
    "# )\n",
    "# df_FinalConcat['years_since_diagnosis'] = pd.to_numeric(df_FinalConcat['years_since_diagnosis'], errors='coerce')\n",
    "# print(\"FinalConcat_Rows: \",len(df_FinalConcat),\"FinalConcat_Columns: \",len(df_FinalConcat.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
